config.production.shortName = "DataRelease"
config.production.eventBrokerHost = "lsst8.ncsa.uiuc.edu"
config.production.productionShutdownTopic = "productionShutdown2"

# note that the database plugin, DatabaseConfigurator, is a placeholder
# and does no interaction with the database.
config.database["db1"].name = "dc3bGlobal"
config.database["db1"].system.authInfo.host = "lsst-db.ncsa.illinois.edu"
config.database["db1"].system.authInfo.port = 3306
config.database["db1"].system.runCleanup.daysFirstNotice = 7
config.database["db1"].system.runCleanup.daysFinalNotice = 1

config.database["db1"].configurationClass = "lsst.ctrl.orca.DatabaseConfigurator"
config.database["db1"].configuration["production"].globalDbName = "GlobalDB"

config.workflow["workflow1"].platform.dir.defaultRoot = "$DEFAULT_ROOT"

config.workflow["workflow1"].platform.deploy.defaultDomain = "$FILE_SYSTEM_DOMAIN"

config.workflow["workflow1"].shutdownTopic = "workflowShutdown2"

config.workflow["workflow1"].configurationType = "condor"
config.workflow["workflow1"].configurationClass = "lsst.ctrl.orca.CondorWorkflowConfigurator"
config.workflow["workflow1"].configuration["condor"].condorData.localScratch = "$LOCAL_SCRATCH"


config.workflow["workflow1"].task["task1"].scriptDir = "workers"

config.workflow["workflow1"].task["task1"].preJob.script.inputFile = "$PLATFORM_DIR/etc/templates/dagman/setups/preJob.sh.template"
config.workflow["workflow1"].task["task1"].preJob.script.outputFile = "preJob.sh"
config.workflow["workflow1"].task["task1"].preJob.condor.inputFile = "$PLATFORM_DIR/etc/templates/dagman/setups/preJob.condor.template"
config.workflow["workflow1"].task["task1"].preJob.condor.keywords["NODE_SET"] = "$NODE_SET"
config.workflow["workflow1"].task["task1"].preJob.condor.keywords["FILE_SYSTEM_DOMAIN"] = "$FILE_SYSTEM_DOMAIN"
config.workflow["workflow1"].task["task1"].preJob.condor.outputFile = "Pipeline.pre"

config.workflow["workflow1"].task["task1"].postJob.script.inputFile = "$PLATFORM_DIR/etc/templates/dagman/setups/postJob.sh.template"
config.workflow["workflow1"].task["task1"].postJob.script.outputFile = "postJob.sh"
config.workflow["workflow1"].task["task1"].postJob.condor.keywords["NODE_SET"] = "$NODE_SET"
config.workflow["workflow1"].task["task1"].postJob.condor.keywords["FILE_SYSTEM_DOMAIN"] = "$FILE_SYSTEM_DOMAIN"
config.workflow["workflow1"].task["task1"].postJob.condor.inputFile = "$PLATFORM_DIR/etc/templates/dagman/setups/postJob.condor.template"
config.workflow["workflow1"].task["task1"].postJob.condor.outputFile = "Pipeline.post"

config.workflow["workflow1"].task["task1"].workerJob.script.inputFile = "$PLATFORM_DIR/etc/templates/dagman/setups/worker.sh.template"
config.workflow["workflow1"].task["task1"].workerJob.script.outputFile = "worker.sh"
config.workflow["workflow1"].task["task1"].workerJob.condor.inputFile = "$PLATFORM_DIR/etc/templates/dagman/setups/workerJob.condor.template"
config.workflow["workflow1"].task["task1"].workerJob.condor.keywords["NODE_SET"] = "$NODE_SET"
config.workflow["workflow1"].task["task1"].workerJob.condor.keywords["FILE_SYSTEM_DOMAIN"] = "$FILE_SYSTEM_DOMAIN"
config.workflow["workflow1"].task["task1"].workerJob.condor.outputFile = "Pipeline-template.condor"



config.workflow["workflow1"].task["task1"].preJob.script.keywords["USERHOME"] = "$USER_HOME"
config.workflow["workflow1"].task["task1"].preJob.script.keywords["USERNAME"] = "$USER_NAME"
config.workflow["workflow1"].task["task1"].preJob.script.keywords["DATADIR"] = "$DATA_DIRECTORY"
config.workflow["workflow1"].task["task1"].preJob.script.keywords["EUPS_PATH"] = "$EUPS_PATH"
config.workflow["workflow1"].task["task1"].preJob.script.keywords["CTRL_EXECUTE_SETUP_PACKAGES"] = "$CTRL_EXECUTE_SETUP_PACKAGES"
config.workflow["workflow1"].task["task1"].preJob.script.keywords["COMMAND"] = "$COMMAND"

config.workflow["workflow1"].task["task1"].workerJob.script.keywords["USERHOME"] = "$USER_HOME"
config.workflow["workflow1"].task["task1"].workerJob.script.keywords["USERNAME"] = "$USER_NAME"
config.workflow["workflow1"].task["task1"].workerJob.script.keywords["DATADIR"] = "$DATA_DIRECTORY"
config.workflow["workflow1"].task["task1"].workerJob.script.keywords["EUPS_PATH"] = "$EUPS_PATH"
config.workflow["workflow1"].task["task1"].workerJob.script.keywords["COMMAND"] = "$COMMAND"


config.workflow["workflow1"].task["task1"].dagGenerator.dagName = "Pipeline"
config.workflow["workflow1"].task["task1"].dagGenerator.script = "$CTRL_EXECUTE_DIR/etc/scripts/generateDag.py"
config.workflow["workflow1"].task["task1"].dagGenerator.idsPerJob = $IDS_PER_JOB
config.workflow["workflow1"].task["task1"].dagGenerator.inputFile = "$INPUT_DATA_FILE"
